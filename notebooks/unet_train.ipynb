{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/cuda/lib/python3.10/site-packages/tqdm-4.65.0-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import torch\n",
    "from spot_master.unet.data import (\n",
    "    FISHSpotsDataset, RandomHorizontalFlip,\n",
    "    RandomRotation, ToTensorWrapper,\n",
    ")\n",
    "from spot_master.unet.model import UNet\n",
    "from spot_master.unet.utils import DiceLoss, RMSELoss\n",
    "from spot_master.unet.train import train\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(),\n",
    "    ToTensorWrapper(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FISHSpotsDataset(\n",
    "    meta_csv=\"meta_train.csv\", root_dir=\"../FISH_spots\",\n",
    "    transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = FISHSpotsDataset(\n",
    "    meta_csv=\"meta_test.csv\", root_dir=\"../FISH_spots\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(1, 1, 4).to(device)\n",
    "# for fine-tuning, if not fine-tuning, comment out the following line\n",
    "model.load_state_dict(torch.load(\"./best_unet_model.pth\"))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "rmse_loss = RMSELoss()\n",
    "dice_loss = DiceLoss()\n",
    "\n",
    "def criterion(pred, target):\n",
    "    loss_dice = dice_loss(pred, target)\n",
    "    loss_rmse = rmse_loss(pred, target)\n",
    "    return 0.6 * loss_dice + 0.4 * loss_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "writer = SummaryWriter(\"runs/unet_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Batch: 1/269, Loss: 0.1395\n",
      "Epoch: 1/10, Batch: 11/269, Loss: 0.1484\n",
      "Epoch: 1/10, Batch: 21/269, Loss: 0.1272\n",
      "Epoch: 1/10, Batch: 31/269, Loss: 0.1438\n",
      "Epoch: 1/10, Batch: 41/269, Loss: 0.1348\n",
      "Epoch: 1/10, Batch: 51/269, Loss: 0.1287\n",
      "Epoch: 1/10, Batch: 61/269, Loss: 0.1383\n",
      "Epoch: 1/10, Batch: 71/269, Loss: 0.1362\n",
      "Epoch: 1/10, Batch: 81/269, Loss: 0.1298\n",
      "Epoch: 1/10, Batch: 91/269, Loss: 0.0981\n",
      "Epoch: 1/10, Batch: 101/269, Loss: 0.1164\n",
      "Epoch: 1/10, Batch: 111/269, Loss: 0.1220\n",
      "Epoch: 1/10, Batch: 121/269, Loss: 0.1541\n",
      "Epoch: 1/10, Batch: 131/269, Loss: 0.1328\n",
      "Epoch: 1/10, Batch: 141/269, Loss: 0.1376\n",
      "Epoch: 1/10, Batch: 151/269, Loss: 0.1161\n",
      "Epoch: 1/10, Batch: 161/269, Loss: 0.1476\n",
      "Epoch: 1/10, Batch: 171/269, Loss: 0.1155\n",
      "Epoch: 1/10, Batch: 181/269, Loss: 0.1079\n",
      "Epoch: 1/10, Batch: 191/269, Loss: 0.1289\n",
      "Epoch: 1/10, Batch: 201/269, Loss: 0.1255\n",
      "Epoch: 1/10, Batch: 211/269, Loss: 0.1311\n",
      "Epoch: 1/10, Batch: 221/269, Loss: 0.1232\n",
      "Epoch: 1/10, Batch: 231/269, Loss: 0.1267\n",
      "Epoch: 1/10, Batch: 241/269, Loss: 0.1415\n",
      "Epoch: 1/10, Batch: 251/269, Loss: 0.1348\n",
      "Epoch: 1/10, Batch: 261/269, Loss: 0.1441\n",
      "Epoch 1/10, Train Loss: 0.1355, Val Loss: 0.1136\n",
      "Best model saved with Val Loss: 0.1136\n",
      "Epoch: 2/10, Batch: 1/269, Loss: 0.1198\n",
      "Epoch: 2/10, Batch: 11/269, Loss: 0.1414\n",
      "Epoch: 2/10, Batch: 21/269, Loss: 0.1486\n",
      "Epoch: 2/10, Batch: 31/269, Loss: 0.1339\n",
      "Epoch: 2/10, Batch: 41/269, Loss: 0.1400\n",
      "Epoch: 2/10, Batch: 51/269, Loss: 0.1295\n",
      "Epoch: 2/10, Batch: 61/269, Loss: 0.1198\n",
      "Epoch: 2/10, Batch: 71/269, Loss: 0.1259\n",
      "Epoch: 2/10, Batch: 81/269, Loss: 0.1220\n",
      "Epoch: 2/10, Batch: 91/269, Loss: 0.1087\n",
      "Epoch: 2/10, Batch: 101/269, Loss: 0.1208\n",
      "Epoch: 2/10, Batch: 111/269, Loss: 0.1378\n",
      "Epoch: 2/10, Batch: 121/269, Loss: 0.1520\n",
      "Epoch: 2/10, Batch: 131/269, Loss: 0.1453\n",
      "Epoch: 2/10, Batch: 141/269, Loss: 0.1443\n",
      "Epoch: 2/10, Batch: 151/269, Loss: 0.1276\n",
      "Epoch: 2/10, Batch: 161/269, Loss: 0.1305\n",
      "Epoch: 2/10, Batch: 171/269, Loss: 0.1121\n",
      "Epoch: 2/10, Batch: 181/269, Loss: 0.1472\n",
      "Epoch: 2/10, Batch: 191/269, Loss: 0.1070\n",
      "Epoch: 2/10, Batch: 201/269, Loss: 0.1187\n",
      "Epoch: 2/10, Batch: 211/269, Loss: 0.1319\n",
      "Epoch: 2/10, Batch: 221/269, Loss: 0.1008\n",
      "Epoch: 2/10, Batch: 231/269, Loss: 0.1089\n",
      "Epoch: 2/10, Batch: 241/269, Loss: 0.1314\n",
      "Epoch: 2/10, Batch: 251/269, Loss: 0.1169\n",
      "Epoch: 2/10, Batch: 261/269, Loss: 0.1101\n",
      "Epoch 2/10, Train Loss: 0.1299, Val Loss: 0.1227\n",
      "Epoch: 3/10, Batch: 1/269, Loss: 0.1196\n",
      "Epoch: 3/10, Batch: 11/269, Loss: 0.1248\n",
      "Epoch: 3/10, Batch: 21/269, Loss: 0.1151\n",
      "Epoch: 3/10, Batch: 31/269, Loss: 0.1252\n",
      "Epoch: 3/10, Batch: 41/269, Loss: 0.1327\n",
      "Epoch: 3/10, Batch: 51/269, Loss: 0.1393\n",
      "Epoch: 3/10, Batch: 61/269, Loss: 0.1508\n",
      "Epoch: 3/10, Batch: 71/269, Loss: 0.1374\n",
      "Epoch: 3/10, Batch: 81/269, Loss: 0.1223\n",
      "Epoch: 3/10, Batch: 91/269, Loss: 0.1259\n",
      "Epoch: 3/10, Batch: 101/269, Loss: 0.1257\n",
      "Epoch: 3/10, Batch: 111/269, Loss: 0.1281\n",
      "Epoch: 3/10, Batch: 121/269, Loss: 0.1317\n",
      "Epoch: 3/10, Batch: 131/269, Loss: 0.1483\n",
      "Epoch: 3/10, Batch: 141/269, Loss: 0.1448\n",
      "Epoch: 3/10, Batch: 151/269, Loss: 0.1284\n",
      "Epoch: 3/10, Batch: 161/269, Loss: 0.1456\n",
      "Epoch: 3/10, Batch: 171/269, Loss: 0.1367\n",
      "Epoch: 3/10, Batch: 181/269, Loss: 0.1503\n",
      "Epoch: 3/10, Batch: 191/269, Loss: 0.1110\n",
      "Epoch: 3/10, Batch: 201/269, Loss: 0.1276\n",
      "Epoch: 3/10, Batch: 211/269, Loss: 0.1375\n",
      "Epoch: 3/10, Batch: 221/269, Loss: 0.1484\n",
      "Epoch: 3/10, Batch: 231/269, Loss: 0.1186\n",
      "Epoch: 3/10, Batch: 241/269, Loss: 0.1175\n",
      "Epoch: 3/10, Batch: 251/269, Loss: 0.1296\n",
      "Epoch: 3/10, Batch: 261/269, Loss: 0.1434\n",
      "Epoch 3/10, Train Loss: 0.1303, Val Loss: 0.1155\n",
      "Epoch: 4/10, Batch: 1/269, Loss: 0.1077\n",
      "Epoch: 4/10, Batch: 11/269, Loss: 0.1327\n",
      "Epoch: 4/10, Batch: 21/269, Loss: 0.1152\n",
      "Epoch: 4/10, Batch: 31/269, Loss: 0.1122\n",
      "Epoch: 4/10, Batch: 41/269, Loss: 0.1079\n",
      "Epoch: 4/10, Batch: 51/269, Loss: 0.1417\n",
      "Epoch: 4/10, Batch: 61/269, Loss: 0.1569\n",
      "Epoch: 4/10, Batch: 71/269, Loss: 0.1339\n",
      "Epoch: 4/10, Batch: 81/269, Loss: 0.1198\n",
      "Epoch: 4/10, Batch: 91/269, Loss: 0.1245\n",
      "Epoch: 4/10, Batch: 101/269, Loss: 0.1270\n",
      "Epoch: 4/10, Batch: 111/269, Loss: 0.1292\n",
      "Epoch: 4/10, Batch: 121/269, Loss: 0.1168\n",
      "Epoch: 4/10, Batch: 131/269, Loss: 0.1216\n",
      "Epoch: 4/10, Batch: 141/269, Loss: 0.1284\n",
      "Epoch: 4/10, Batch: 151/269, Loss: 0.1320\n",
      "Epoch: 4/10, Batch: 161/269, Loss: 0.1257\n",
      "Epoch: 4/10, Batch: 171/269, Loss: 0.1162\n",
      "Epoch: 4/10, Batch: 181/269, Loss: 0.1508\n",
      "Epoch: 4/10, Batch: 191/269, Loss: 0.1277\n",
      "Epoch: 4/10, Batch: 201/269, Loss: 0.1122\n",
      "Epoch: 4/10, Batch: 211/269, Loss: 0.1544\n",
      "Epoch: 4/10, Batch: 221/269, Loss: 0.1274\n",
      "Epoch: 4/10, Batch: 231/269, Loss: 0.0968\n",
      "Epoch: 4/10, Batch: 241/269, Loss: 0.1345\n",
      "Epoch: 4/10, Batch: 251/269, Loss: 0.1451\n",
      "Epoch: 4/10, Batch: 261/269, Loss: 0.1388\n",
      "Epoch 4/10, Train Loss: 0.1314, Val Loss: 0.1173\n",
      "Epoch: 5/10, Batch: 1/269, Loss: 0.1034\n",
      "Epoch: 5/10, Batch: 11/269, Loss: 0.1519\n",
      "Epoch: 5/10, Batch: 21/269, Loss: 0.1268\n",
      "Epoch: 5/10, Batch: 31/269, Loss: 0.1215\n",
      "Epoch: 5/10, Batch: 41/269, Loss: 0.1487\n",
      "Epoch: 5/10, Batch: 51/269, Loss: 0.1218\n",
      "Epoch: 5/10, Batch: 61/269, Loss: 0.1270\n",
      "Epoch: 5/10, Batch: 71/269, Loss: 0.1293\n",
      "Epoch: 5/10, Batch: 81/269, Loss: 0.1265\n",
      "Epoch: 5/10, Batch: 91/269, Loss: 0.1480\n",
      "Epoch: 5/10, Batch: 101/269, Loss: 0.1362\n",
      "Epoch: 5/10, Batch: 111/269, Loss: 0.1220\n",
      "Epoch: 5/10, Batch: 121/269, Loss: 0.1482\n",
      "Epoch: 5/10, Batch: 131/269, Loss: 0.1049\n",
      "Epoch: 5/10, Batch: 141/269, Loss: 0.1112\n",
      "Epoch: 5/10, Batch: 151/269, Loss: 0.1311\n",
      "Epoch: 5/10, Batch: 161/269, Loss: 0.1180\n",
      "Epoch: 5/10, Batch: 171/269, Loss: 0.1301\n",
      "Epoch: 5/10, Batch: 181/269, Loss: 0.1366\n",
      "Epoch: 5/10, Batch: 191/269, Loss: 0.1247\n",
      "Epoch: 5/10, Batch: 201/269, Loss: 0.1472\n",
      "Epoch: 5/10, Batch: 211/269, Loss: 0.1208\n",
      "Epoch: 5/10, Batch: 221/269, Loss: 0.1435\n",
      "Epoch: 5/10, Batch: 231/269, Loss: 0.1357\n",
      "Epoch: 5/10, Batch: 241/269, Loss: 0.1241\n",
      "Epoch: 5/10, Batch: 251/269, Loss: 0.1013\n",
      "Epoch: 5/10, Batch: 261/269, Loss: 0.1155\n",
      "Epoch 5/10, Train Loss: 0.1285, Val Loss: 0.1122\n",
      "Best model saved with Val Loss: 0.1122\n",
      "Epoch: 6/10, Batch: 1/269, Loss: 0.1421\n",
      "Epoch: 6/10, Batch: 11/269, Loss: 0.1308\n",
      "Epoch: 6/10, Batch: 21/269, Loss: 0.1317\n",
      "Epoch: 6/10, Batch: 31/269, Loss: 0.1140\n",
      "Epoch: 6/10, Batch: 41/269, Loss: 0.1210\n",
      "Epoch: 6/10, Batch: 51/269, Loss: 0.1351\n",
      "Epoch: 6/10, Batch: 61/269, Loss: 0.1250\n",
      "Epoch: 6/10, Batch: 71/269, Loss: 0.1348\n",
      "Epoch: 6/10, Batch: 81/269, Loss: 0.1094\n",
      "Epoch: 6/10, Batch: 91/269, Loss: 0.1376\n",
      "Epoch: 6/10, Batch: 101/269, Loss: 0.1390\n",
      "Epoch: 6/10, Batch: 111/269, Loss: 0.1293\n",
      "Epoch: 6/10, Batch: 121/269, Loss: 0.1544\n",
      "Epoch: 6/10, Batch: 131/269, Loss: 0.1339\n",
      "Epoch: 6/10, Batch: 141/269, Loss: 0.1604\n",
      "Epoch: 6/10, Batch: 151/269, Loss: 0.1091\n",
      "Epoch: 6/10, Batch: 161/269, Loss: 0.1165\n",
      "Epoch: 6/10, Batch: 171/269, Loss: 0.1458\n",
      "Epoch: 6/10, Batch: 181/269, Loss: 0.1450\n",
      "Epoch: 6/10, Batch: 191/269, Loss: 0.1064\n",
      "Epoch: 6/10, Batch: 201/269, Loss: 0.1230\n",
      "Epoch: 6/10, Batch: 211/269, Loss: 0.1472\n",
      "Epoch: 6/10, Batch: 221/269, Loss: 0.1222\n",
      "Epoch: 6/10, Batch: 231/269, Loss: 0.1256\n",
      "Epoch: 6/10, Batch: 241/269, Loss: 0.1557\n",
      "Epoch: 6/10, Batch: 251/269, Loss: 0.1532\n",
      "Epoch: 6/10, Batch: 261/269, Loss: 0.1233\n",
      "Epoch 6/10, Train Loss: 0.1282, Val Loss: 0.1056\n",
      "Best model saved with Val Loss: 0.1056\n",
      "Epoch: 7/10, Batch: 1/269, Loss: 0.1251\n",
      "Epoch: 7/10, Batch: 11/269, Loss: 0.1269\n",
      "Epoch: 7/10, Batch: 21/269, Loss: 0.0958\n",
      "Epoch: 7/10, Batch: 31/269, Loss: 0.1227\n",
      "Epoch: 7/10, Batch: 41/269, Loss: 0.1223\n",
      "Epoch: 7/10, Batch: 51/269, Loss: 0.1120\n",
      "Epoch: 7/10, Batch: 61/269, Loss: 0.1312\n",
      "Epoch: 7/10, Batch: 71/269, Loss: 0.1247\n",
      "Epoch: 7/10, Batch: 81/269, Loss: 0.1492\n",
      "Epoch: 7/10, Batch: 91/269, Loss: 0.1369\n",
      "Epoch: 7/10, Batch: 101/269, Loss: 0.1323\n",
      "Epoch: 7/10, Batch: 111/269, Loss: 0.1451\n",
      "Epoch: 7/10, Batch: 121/269, Loss: 0.1575\n",
      "Epoch: 7/10, Batch: 131/269, Loss: 0.1445\n",
      "Epoch: 7/10, Batch: 141/269, Loss: 0.1381\n",
      "Epoch: 7/10, Batch: 151/269, Loss: 0.1400\n",
      "Epoch: 7/10, Batch: 161/269, Loss: 0.1317\n",
      "Epoch: 7/10, Batch: 171/269, Loss: 0.1293\n",
      "Epoch: 7/10, Batch: 181/269, Loss: 0.1030\n",
      "Epoch: 7/10, Batch: 191/269, Loss: 0.1176\n",
      "Epoch: 7/10, Batch: 201/269, Loss: 0.1387\n",
      "Epoch: 7/10, Batch: 211/269, Loss: 0.1144\n",
      "Epoch: 7/10, Batch: 221/269, Loss: 0.1527\n",
      "Epoch: 7/10, Batch: 231/269, Loss: 0.1276\n",
      "Epoch: 7/10, Batch: 241/269, Loss: 0.0985\n",
      "Epoch: 7/10, Batch: 251/269, Loss: 0.1307\n",
      "Epoch: 7/10, Batch: 261/269, Loss: 0.1110\n",
      "Epoch 7/10, Train Loss: 0.1282, Val Loss: 0.1073\n",
      "Epoch: 8/10, Batch: 1/269, Loss: 0.1057\n",
      "Epoch: 8/10, Batch: 11/269, Loss: 0.1237\n",
      "Epoch: 8/10, Batch: 21/269, Loss: 0.1666\n",
      "Epoch: 8/10, Batch: 31/269, Loss: 0.1353\n",
      "Epoch: 8/10, Batch: 41/269, Loss: 0.1288\n",
      "Epoch: 8/10, Batch: 51/269, Loss: 0.1164\n",
      "Epoch: 8/10, Batch: 61/269, Loss: 0.1126\n",
      "Epoch: 8/10, Batch: 71/269, Loss: 0.1217\n",
      "Epoch: 8/10, Batch: 81/269, Loss: 0.1120\n",
      "Epoch: 8/10, Batch: 91/269, Loss: 0.1071\n",
      "Epoch: 8/10, Batch: 101/269, Loss: 0.1244\n",
      "Epoch: 8/10, Batch: 111/269, Loss: 0.1258\n",
      "Epoch: 8/10, Batch: 121/269, Loss: 0.1073\n",
      "Epoch: 8/10, Batch: 131/269, Loss: 0.1661\n",
      "Epoch: 8/10, Batch: 141/269, Loss: 0.1457\n",
      "Epoch: 8/10, Batch: 151/269, Loss: 0.1249\n",
      "Epoch: 8/10, Batch: 161/269, Loss: 0.1192\n",
      "Epoch: 8/10, Batch: 171/269, Loss: 0.1260\n",
      "Epoch: 8/10, Batch: 181/269, Loss: 0.1324\n",
      "Epoch: 8/10, Batch: 191/269, Loss: 0.1364\n",
      "Epoch: 8/10, Batch: 201/269, Loss: 0.1310\n",
      "Epoch: 8/10, Batch: 211/269, Loss: 0.1334\n",
      "Epoch: 8/10, Batch: 221/269, Loss: 0.1295\n",
      "Epoch: 8/10, Batch: 231/269, Loss: 0.1363\n",
      "Epoch: 8/10, Batch: 241/269, Loss: 0.1246\n",
      "Epoch: 8/10, Batch: 251/269, Loss: 0.1299\n",
      "Epoch: 8/10, Batch: 261/269, Loss: 0.1356\n",
      "Epoch 8/10, Train Loss: 0.1279, Val Loss: 0.1114\n",
      "Epoch: 9/10, Batch: 1/269, Loss: 0.1284\n",
      "Epoch: 9/10, Batch: 11/269, Loss: 0.1268\n",
      "Epoch: 9/10, Batch: 21/269, Loss: 0.1076\n",
      "Epoch: 9/10, Batch: 31/269, Loss: 0.1176\n",
      "Epoch: 9/10, Batch: 41/269, Loss: 0.1149\n",
      "Epoch: 9/10, Batch: 51/269, Loss: 0.1218\n",
      "Epoch: 9/10, Batch: 61/269, Loss: 0.1158\n",
      "Epoch: 9/10, Batch: 71/269, Loss: 0.1215\n",
      "Epoch: 9/10, Batch: 81/269, Loss: 0.1443\n",
      "Epoch: 9/10, Batch: 91/269, Loss: 0.1500\n",
      "Epoch: 9/10, Batch: 101/269, Loss: 0.2249\n",
      "Epoch: 9/10, Batch: 111/269, Loss: 0.1225\n",
      "Epoch: 9/10, Batch: 121/269, Loss: 0.1216\n",
      "Epoch: 9/10, Batch: 131/269, Loss: 0.1229\n",
      "Epoch: 9/10, Batch: 141/269, Loss: 0.1062\n",
      "Epoch: 9/10, Batch: 151/269, Loss: 0.1354\n",
      "Epoch: 9/10, Batch: 161/269, Loss: 0.1190\n",
      "Epoch: 9/10, Batch: 171/269, Loss: 0.1319\n",
      "Epoch: 9/10, Batch: 181/269, Loss: 0.1167\n",
      "Epoch: 9/10, Batch: 191/269, Loss: 0.1566\n",
      "Epoch: 9/10, Batch: 201/269, Loss: 0.1394\n",
      "Epoch: 9/10, Batch: 211/269, Loss: 0.1152\n",
      "Epoch: 9/10, Batch: 221/269, Loss: 0.1556\n",
      "Epoch: 9/10, Batch: 231/269, Loss: 0.1522\n",
      "Epoch: 9/10, Batch: 241/269, Loss: 0.1153\n",
      "Epoch: 9/10, Batch: 251/269, Loss: 0.1235\n",
      "Epoch: 9/10, Batch: 261/269, Loss: 0.1137\n",
      "Epoch 9/10, Train Loss: 0.1279, Val Loss: 0.1101\n",
      "Epoch: 10/10, Batch: 1/269, Loss: 0.1511\n",
      "Epoch: 10/10, Batch: 11/269, Loss: 0.1309\n",
      "Epoch: 10/10, Batch: 21/269, Loss: 0.1330\n",
      "Epoch: 10/10, Batch: 31/269, Loss: 0.1186\n",
      "Epoch: 10/10, Batch: 41/269, Loss: 0.1218\n",
      "Epoch: 10/10, Batch: 51/269, Loss: 0.1401\n",
      "Epoch: 10/10, Batch: 61/269, Loss: 0.1393\n",
      "Epoch: 10/10, Batch: 71/269, Loss: 0.1251\n",
      "Epoch: 10/10, Batch: 81/269, Loss: 0.1565\n",
      "Epoch: 10/10, Batch: 91/269, Loss: 0.1211\n",
      "Epoch: 10/10, Batch: 101/269, Loss: 0.1416\n",
      "Epoch: 10/10, Batch: 111/269, Loss: 0.0954\n",
      "Epoch: 10/10, Batch: 121/269, Loss: 0.0944\n",
      "Epoch: 10/10, Batch: 131/269, Loss: 0.1271\n",
      "Epoch: 10/10, Batch: 141/269, Loss: 0.1372\n",
      "Epoch: 10/10, Batch: 151/269, Loss: 0.1253\n",
      "Epoch: 10/10, Batch: 161/269, Loss: 0.1209\n",
      "Epoch: 10/10, Batch: 171/269, Loss: 0.1153\n",
      "Epoch: 10/10, Batch: 181/269, Loss: 0.1351\n",
      "Epoch: 10/10, Batch: 191/269, Loss: 0.1200\n",
      "Epoch: 10/10, Batch: 201/269, Loss: 0.1706\n",
      "Epoch: 10/10, Batch: 211/269, Loss: 0.1326\n",
      "Epoch: 10/10, Batch: 221/269, Loss: 0.1362\n",
      "Epoch: 10/10, Batch: 231/269, Loss: 0.1381\n",
      "Epoch: 10/10, Batch: 241/269, Loss: 0.1220\n",
      "Epoch: 10/10, Batch: 251/269, Loss: 0.1169\n",
      "Epoch: 10/10, Batch: 261/269, Loss: 0.1541\n",
      "Epoch 10/10, Train Loss: 0.1272, Val Loss: 0.1093\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model, optimizer, criterion, writer, device,\n",
    "    train_loader, test_loader,\n",
    "    \"best_unet_model_after_fine_tuning.pth\", num_epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
